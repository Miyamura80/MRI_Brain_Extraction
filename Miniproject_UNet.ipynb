{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTsGGMpEq00A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPNlifgHrdin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neptune\n"
      ],
      "metadata": {
        "id": "9e6ow9t8wnCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq0_DaIcrdlC",
        "outputId": "4871fb17-8624-4aed-b74b-14a37413af3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neptune\n",
            "  Downloading neptune-1.1.1-py3-none-any.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.26.103-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyJWT\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (2.27.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from neptune) (5.9.4)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (1.16.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (8.1.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from neptune) (1.26.15)\n",
            "Collecting swagger-spec-validator>=2.7.4\n",
            "  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neptune) (1.4.4)\n",
            "Collecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.9/dist-packages (from neptune) (8.4.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from neptune) (0.18.3)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from neptune) (23.0)\n",
            "Collecting bravado<12.0.0,>=11.0.0\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.103\n",
            "  Downloading botocore-1.29.103-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson\n",
            "  Downloading simplejson-3.18.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.5.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.1-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->neptune) (2022.12.7)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.3.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neptune) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas->neptune) (1.22.4)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (22.2.0)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.9/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rfc3987, monotonic, websocket-client, uri-template, smmap, simplejson, rfc3339-validator, PyJWT, jsonref, jsonpointer, jmespath, fqdn, swagger-spec-validator, gitdb, botocore, arrow, s3transfer, isoduration, GitPython, boto3, bravado-core, bravado, neptune\n",
            "Successfully installed GitPython-3.1.31 PyJWT-2.6.0 arrow-1.2.3 boto3-1.26.103 botocore-1.29.103 bravado-11.0.3 bravado-core-5.17.1 fqdn-1.5.1 gitdb-4.0.10 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.3 jsonref-1.1.0 monotonic-1.6 neptune-1.1.1 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.6.0 simplejson-3.18.4 smmap-5.0.0 swagger-spec-validator-3.0.3 uri-template-1.2.0 websocket-client-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import neptune\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pSMP5XvIy-dD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n"
      ],
      "metadata": {
        "id": "xN1VrshZtlk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4ZFuPXordnZ",
        "outputId": "0d3cdfa8-47be-40bd-8eca-7d0c06bbcf79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "X_train = np.load('/content/drive/MyDrive/healthcare_practicals/miniproject/data/X_guys_2d.npy')\n",
        "y_train = np.load('/content/drive/MyDrive/healthcare_practicals/miniproject/data/y_guys_128_2d.npy')\n",
        "print(X_train.shape, y_train.shape)\n",
        "\n",
        "\n",
        "X_train, valtest_images, y_train, valtest_labels = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(valtest_images, valtest_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
        "X_val = X_val.reshape(X_val.shape[0],1,X_val.shape[1],X_val.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1],X_test.shape[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9AtrNuptnDY",
        "outputId": "1abb8575-ae26-41bb-f41a-fd52ab7338be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(317, 128, 128) (317, 128, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(np.average(X_train))\n",
        "print(np.average(X_val))\n",
        "print(np.average(X_test))\n",
        "\n",
        "print(np.std(X_train))\n",
        "print(np.std(X_val))\n",
        "print(np.std(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgjGR1kcIdCR",
        "outputId": "73329a59-9d70-4b5e-df9c-72988bbffd6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(253, 1, 128, 128)\n",
            "(32, 1, 128, 128)\n",
            "(32, 1, 128, 128)\n",
            "287.9302712420177\n",
            "268.7814110944717\n",
            "316.16472715740406\n",
            "367.3290279231455\n",
            "334.4379147954599\n",
            "398.5666321580297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class numpy_dataset(Dataset):  # Inherit from Dataset class\n",
        "    def __init__(self, data, target):\n",
        "        ## Add code here \n",
        "        self.data = torch.from_numpy(data)\n",
        "        self.target = torch.from_numpy(target)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "train_dataset = numpy_dataset(X_train, y_train)\n",
        "val_dataset = numpy_dataset(X_val, y_val)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "NHGB9L4gtnFw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet\n"
      ],
      "metadata": {
        "id": "Nm_cCkc-rhFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, init_features=4, out_channels=2):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(1, features, \"encoder1\")\n",
        "        self.pool1 = nn.MaxPool2d(2, stride=3)\n",
        "        self.encoder2 = UNet._block(features, 2*features, \"encoder2\")\n",
        "        self.pool2 = nn.MaxPool2d(2, stride=3)\n",
        "        self.bottleneck = UNet._block(2*features, 4*features, \"bottleneck\")\n",
        "        self.upconv2 = nn.Upsample(size=(43, 43), mode='nearest')\n",
        "        self.decoder2 = UNet._block(4*features + 2*features, 2*features, \"decoder2\")\n",
        "        self.upconv1 = nn.Upsample(size=(128, 128), mode='nearest')\n",
        "        self.decoder1 = UNet._block(2*features + features, features, \"decoder1\")\n",
        "        self.conv = nn.Conv2d(features, in_channels, kernel_size=(3,3), padding=1)\n",
        "        self.activation = torch.sigmoid\n",
        "        self.double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.bottleneck(self.pool2(enc2))\n",
        "        upconv2 = self.upconv2(enc3)\n",
        "\n",
        "        concat_1 = torch.cat([enc2, upconv2], dim=1)\n",
        "        dec1 = self.decoder2(concat_1)\n",
        "        dec2 = self.decoder1(torch.cat([enc1, self.upconv1(dec1)], dim=1))\n",
        "        last_layer = self.conv(dec2)\n",
        "        output = self.activation(last_layer)\n",
        "        return torch.squeeze(output)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, kernel_size=(3,3), padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm2d(features),\n",
        "        )"
      ],
      "metadata": {
        "id": "hsLd1zqcrdqB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L2 Regularization"
      ],
      "metadata": {
        "id": "Wv2IpBMkJTE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class L2RegLoss(nn.Module):\n",
        "    def __init__(self, loss_fn=None, mu=1):\n",
        "        super(L2RegLoss, self).__init__()\n",
        "        self.eps = 1e-7\n",
        "        self.mu = mu\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def forward(self, x, target, model):\n",
        "        main_loss = self.loss_fn(x, target)\n",
        "        reg_loss = torch.mean(torch.stack([p.norm()**2 for p in model.parameters()]))\n",
        "        return main_loss + self.mu * reg_loss\n",
        "    "
      ],
      "metadata": {
        "id": "AFFRTukdJUs2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, dataloader, optim, loss_fn, epoch):\n",
        "    start_time = time.perf_counter()\n",
        "    net.train()  #Put the network in train mode\n",
        "    total_loss = 0\n",
        "    batches = 0\n",
        "    pred_store = []\n",
        "    true_store = []\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        batches += 1\n",
        "\n",
        "        # Training loop\n",
        "        pred = net(data)\n",
        "        loss = loss_fn(pred, target, net)\n",
        "        net.zero_grad()\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        total_loss += loss\n",
        "        pred_store.append(np.argmax(pred.detach().numpy(), axis=1))\n",
        "        true_store.append(np.argmax(target.detach().numpy(), axis=1))\n",
        "\n",
        "\n",
        "        if batch_idx % 100 == 0: #Report stats every x batches\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, (batch_idx+1) * len(data), len(dataloader.dataset),\n",
        "                           100. * (batch_idx+1) / len(dataloader), loss.item()), flush=True)\n",
        "    av_loss = total_loss / batches\n",
        "    av_loss = av_loss.detach().cpu().numpy()\n",
        "    print('\\nTraining set: Average loss: {:.4f}'.format(av_loss,  flush=True))\n",
        "    total_time = time.perf_counter() - start_time\n",
        "\n",
        "    model_weight = torch.mean(torch.stack([p.norm() ** 2 for p in net.parameters()]))\n",
        "\n",
        "    print('Time taken for epoch = ', total_time)\n",
        "    return av_loss, model_weight, (data,pred,target)\n",
        "\n",
        "def val(net, val_dataloader, optim, loss_fn, epoch):\n",
        "    net.eval()  #Put the model in eval mode\n",
        "    total_loss = 0    \n",
        "    pred_store = []\n",
        "    true_store = []\n",
        "    batches = 0\n",
        "    with torch.no_grad():  # So no gradients accumulate\n",
        "        for batch_idx, (data, target) in enumerate(val_dataloader):\n",
        "            batches += 1\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            # Eval steps\n",
        "            pred = net(data)\n",
        "            loss =  loss_fn(pred, target, net)\n",
        "\n",
        "            total_loss += loss\n",
        "            pred_store.append(np.argmax(pred.detach().numpy(), axis=1))\n",
        "            true_store.append(np.argmax(target.detach().numpy(), axis=1))\n",
        "        av_loss = total_loss / batches\n",
        "        \n",
        "    av_loss = av_loss.detach().numpy()\n",
        "\n",
        "    pred_store = np.array(pred_store).reshape(-1)\n",
        "    true_store = np.array(true_store).reshape(-1)\n",
        "    acc = accuracy_score(pred_store, true_store)\n",
        "        \n",
        "    print('Validation set: Average loss: {:.4f}'.format(av_loss,  flush=True))\n",
        "    print('\\n')\n",
        "    return av_loss\n",
        "\n",
        "\n",
        "def predict(net, test_dataloader):\n",
        "    pred_store = []\n",
        "    true_store = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, target in test_dataloader:            \n",
        "            pred = net(inputs)\n",
        "\n",
        "            pred_store.append(np.argmax(pred.detach().numpy(), axis=1))\n",
        "            true_store.append(np.argmax(target.detach().numpy(), axis=1))\n",
        "    \n",
        "    pred_store = np.array(pred_store).reshape(-1)\n",
        "    true_store = np.array(true_store).reshape(-1)\n",
        "    \n",
        "    return pred_store, true_store\n"
      ],
      "metadata": {
        "id": "c7j21vQYsvSH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dice_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(dice_loss, self).__init__()\n",
        "        self.eps=1e-7\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        num_classes = target.shape[1]   # Channels first\n",
        "        target = target.type(x.type())\n",
        "        dims = (0,) + tuple(range(2, target.ndimension()))\n",
        "        intersection = torch.sum(x * target, dims)\n",
        "        cardinality = torch.sum(x + target, dims)\n",
        "        dice_loss = (2. * intersection / (cardinality + self.eps)).mean()\n",
        "        return 1-dice_loss"
      ],
      "metadata": {
        "id": "0xNKXs3w2Fmm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "wJEDy2PF9ILG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_dataset = numpy_dataset(X_train, y_train)\n",
        "val_dataset = numpy_dataset(X_val, y_val)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=True)\n",
        "\n",
        "net = UNet(in_channels=1, init_features=4, out_channels=2)\n",
        "\n",
        "# Calculate the number of traininable params\n",
        "# print('Trainable params: ', params)\n",
        "\n",
        "\n",
        "# Neptune\n",
        "run = neptune.init_run(\n",
        "    capture_hardware_metrics=True,\n",
        "    capture_stderr=True,\n",
        "    capture_stdout=True,\n",
        ")  \n",
        "\n",
        "params = {\"lr\": 0.05, \"optimizer\": \"SGD\", \"loss\": \"dice\", \"epoch\": 150, \"mu\": 1, \"model\": \"UNet\"}\n",
        "run[\"parameters\"] = params\n",
        "\n",
        "\n",
        "loss_dict = {\"bce\": nn.BCELoss(), \"dice\": dice_loss()}\n",
        "optim_dict = {\"Adam\": torch.optim.Adam(net.parameters(), lr = params[\"lr\"]),\n",
        "              \"SGD\" : torch.optim.SGD(net.parameters(), lr = params[\"lr\"])\n",
        "              }\n",
        "\n",
        "\n",
        "class_loss = loss_dict[params[\"loss\"]]\n",
        "# Mu to vary between 1, 0.1\n",
        "class_loss = L2RegLoss(class_loss, mu=params[\"mu\"])\n",
        "\n",
        "optim = optim_dict[params[\"optimizer\"]]\n",
        "\n",
        "losses = []\n",
        "max_epochs = params[\"epoch\"]\n",
        "for epoch in range(1, max_epochs+1):\n",
        "    train_loss, model_weight, (last_data,last_pred,last_target) = train(net, train_dataloader, optim, class_loss, epoch)\n",
        "    val_loss = val(net, val_dataloader, optim, class_loss, epoch)\n",
        "\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        brain_img = Image.fromarray(last_data[0].squeeze(0).cpu().numpy()).convert(\"L\")\n",
        "\n",
        "        seg_img = last_pred[0].detach().numpy()*255\n",
        "        seg_img = Image.fromarray(seg_img).convert(\"L\")\n",
        "\n",
        "        target_img = last_target[0].detach().numpy()*255\n",
        "        target_img = Image.fromarray(target_img).convert(\"L\")\n",
        "        # Neptune: Image log\n",
        "        run[\"train/images\"].append(brain_img, description=f\"Epoch {epoch}: Input Img \\n Avg val loss: {val_loss}\")\n",
        "        run[\"train/images\"].append(seg_img, description=f\"Epoch {epoch}: Output Img \\n Avg val loss: {val_loss}\") \n",
        "        run[\"train/images\"].append(target_img, description=f\"Epoch {epoch}: Target Img \\n Avg val loss: {val_loss}\") \n",
        "        run[\"train/temp_segmentation_average\"].append(last_pred[0].mean())\n",
        "\n",
        "    # Neptune: Loss logging\n",
        "    run[\"train/train_loss\"].append(train_loss)\n",
        "    run[\"train/val_loss\"].append(val_loss)\n",
        "    run[\"train/model_weight\"].append(model_weight)\n",
        "\n",
        "\n",
        "torch.save(net.state_dict(), \"UNet.pth\")\n",
        "run[\"model_checkpoint/final_model\"].upload(\"UNet.pth\")\n",
        "\n",
        "\n",
        "run.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQcBdXJ0xirs",
        "outputId": "e838d9f2-96ad-4b5e-e7f1-4e7b0e63a7fb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/miyamura80/DLH-Miniproject/e/DLHMIN-63\n",
            "Train Epoch: 1 [16/253 (7%)]\tLoss: 3.088343\n",
            "\n",
            "Training set: Average loss: 2.8907\n",
            "Time taken for epoch =  12.41242613500026\n",
            "Validation set: Average loss: 2.7476\n",
            "\n",
            "\n",
            "Train Epoch: 2 [16/253 (7%)]\tLoss: 2.655752\n",
            "\n",
            "Training set: Average loss: 2.4967\n",
            "Time taken for epoch =  11.77878205800016\n",
            "Validation set: Average loss: 2.3982\n",
            "\n",
            "\n",
            "Train Epoch: 3 [16/253 (7%)]\tLoss: 2.330768\n",
            "\n",
            "Training set: Average loss: 2.1958\n",
            "Time taken for epoch =  11.714813083000081\n",
            "Validation set: Average loss: 2.1088\n",
            "\n",
            "\n",
            "Train Epoch: 4 [16/253 (7%)]\tLoss: 2.042505\n",
            "\n",
            "Training set: Average loss: 1.9424\n",
            "Time taken for epoch =  11.97182513000007\n",
            "Validation set: Average loss: 1.8621\n",
            "\n",
            "\n",
            "Train Epoch: 5 [16/253 (7%)]\tLoss: 1.827926\n",
            "\n",
            "Training set: Average loss: 1.7355\n",
            "Time taken for epoch =  21.276662769999803\n",
            "Validation set: Average loss: 1.6698\n",
            "\n",
            "\n",
            "Train Epoch: 6 [16/253 (7%)]\tLoss: 1.615238\n",
            "\n",
            "Training set: Average loss: 1.5516\n",
            "Time taken for epoch =  11.390977943000053\n",
            "Validation set: Average loss: 1.4925\n",
            "\n",
            "\n",
            "Train Epoch: 7 [16/253 (7%)]\tLoss: 1.463630\n",
            "\n",
            "Training set: Average loss: 1.3953\n",
            "Time taken for epoch =  11.172564702999807\n",
            "Validation set: Average loss: 1.3462\n",
            "\n",
            "\n",
            "Train Epoch: 8 [16/253 (7%)]\tLoss: 1.318410\n",
            "\n",
            "Training set: Average loss: 1.2619\n",
            "Time taken for epoch =  12.218345927999962\n",
            "Validation set: Average loss: 1.2564\n",
            "\n",
            "\n",
            "Train Epoch: 9 [16/253 (7%)]\tLoss: 1.191023\n",
            "\n",
            "Training set: Average loss: 1.1497\n",
            "Time taken for epoch =  12.147708335000061\n",
            "Validation set: Average loss: 1.1091\n",
            "\n",
            "\n",
            "Train Epoch: 10 [16/253 (7%)]\tLoss: 1.069998\n",
            "\n",
            "Training set: Average loss: 1.0771\n",
            "Time taken for epoch =  10.938843505000023\n",
            "Validation set: Average loss: 1.0274\n",
            "\n",
            "\n",
            "Train Epoch: 11 [16/253 (7%)]\tLoss: 0.991925\n",
            "\n",
            "Training set: Average loss: 0.9578\n",
            "Time taken for epoch =  11.174603222999849\n",
            "Validation set: Average loss: 1.0135\n",
            "\n",
            "\n",
            "Train Epoch: 12 [16/253 (7%)]\tLoss: 0.915166\n",
            "\n",
            "Training set: Average loss: 0.8872\n",
            "Time taken for epoch =  11.668384840999806\n",
            "Validation set: Average loss: 0.8784\n",
            "\n",
            "\n",
            "Train Epoch: 13 [16/253 (7%)]\tLoss: 0.846141\n",
            "\n",
            "Training set: Average loss: 0.8099\n",
            "Time taken for epoch =  11.677188150000347\n",
            "Validation set: Average loss: 0.8095\n",
            "\n",
            "\n",
            "Train Epoch: 14 [16/253 (7%)]\tLoss: 0.772375\n",
            "\n",
            "Training set: Average loss: 0.7468\n",
            "Time taken for epoch =  12.269325000000208\n",
            "Validation set: Average loss: 0.8356\n",
            "\n",
            "\n",
            "Train Epoch: 15 [16/253 (7%)]\tLoss: 0.701481\n",
            "\n",
            "Training set: Average loss: 0.7601\n",
            "Time taken for epoch =  12.928861571000198\n",
            "Validation set: Average loss: 0.7721\n",
            "\n",
            "\n",
            "Train Epoch: 16 [16/253 (7%)]\tLoss: 0.684295\n",
            "\n",
            "Training set: Average loss: 0.6803\n",
            "Time taken for epoch =  11.855334448000121\n",
            "Validation set: Average loss: 0.8320\n",
            "\n",
            "\n",
            "Train Epoch: 17 [16/253 (7%)]\tLoss: 0.699108\n",
            "\n",
            "Training set: Average loss: 0.6539\n",
            "Time taken for epoch =  11.043201294000028\n",
            "Validation set: Average loss: 1.3234\n",
            "\n",
            "\n",
            "Train Epoch: 18 [16/253 (7%)]\tLoss: 0.723258\n",
            "\n",
            "Training set: Average loss: 0.6532\n",
            "Time taken for epoch =  11.567362245999902\n",
            "Validation set: Average loss: 0.6879\n",
            "\n",
            "\n",
            "Train Epoch: 19 [16/253 (7%)]\tLoss: 0.648971\n",
            "\n",
            "Training set: Average loss: 0.5881\n",
            "Time taken for epoch =  11.879131585000323\n",
            "Validation set: Average loss: 0.6932\n",
            "\n",
            "\n",
            "Train Epoch: 20 [16/253 (7%)]\tLoss: 0.543794\n",
            "\n",
            "Training set: Average loss: 0.5383\n",
            "Time taken for epoch =  11.498448016999646\n",
            "Validation set: Average loss: 0.5898\n",
            "\n",
            "\n",
            "Train Epoch: 21 [16/253 (7%)]\tLoss: 0.519091\n",
            "\n",
            "Training set: Average loss: 0.6018\n",
            "Time taken for epoch =  10.902741224000238\n",
            "Validation set: Average loss: 0.7068\n",
            "\n",
            "\n",
            "Train Epoch: 22 [16/253 (7%)]\tLoss: 0.635519\n",
            "\n",
            "Training set: Average loss: 0.6216\n",
            "Time taken for epoch =  11.486602292000043\n",
            "Validation set: Average loss: 0.6017\n",
            "\n",
            "\n",
            "Train Epoch: 23 [16/253 (7%)]\tLoss: 0.539976\n",
            "\n",
            "Training set: Average loss: 0.5669\n",
            "Time taken for epoch =  10.137274843000341\n",
            "Validation set: Average loss: 0.5770\n",
            "\n",
            "\n",
            "Train Epoch: 24 [16/253 (7%)]\tLoss: 0.527005\n",
            "\n",
            "Training set: Average loss: 0.5399\n",
            "Time taken for epoch =  12.060961071999827\n",
            "Validation set: Average loss: 0.5567\n",
            "\n",
            "\n",
            "Train Epoch: 25 [16/253 (7%)]\tLoss: 0.518662\n",
            "\n",
            "Training set: Average loss: 0.5419\n",
            "Time taken for epoch =  11.681746793000002\n",
            "Validation set: Average loss: 0.8332\n",
            "\n",
            "\n",
            "Train Epoch: 26 [16/253 (7%)]\tLoss: 0.538646\n",
            "\n",
            "Training set: Average loss: 0.5042\n",
            "Time taken for epoch =  12.089647643000262\n",
            "Validation set: Average loss: 0.5322\n",
            "\n",
            "\n",
            "Train Epoch: 27 [16/253 (7%)]\tLoss: 0.459694\n",
            "\n",
            "Training set: Average loss: 0.4854\n",
            "Time taken for epoch =  11.874499408000247\n",
            "Validation set: Average loss: 0.5954\n",
            "\n",
            "\n",
            "Train Epoch: 28 [16/253 (7%)]\tLoss: 0.562911\n",
            "\n",
            "Training set: Average loss: 0.4886\n",
            "Time taken for epoch =  10.153284067000186\n",
            "Validation set: Average loss: 0.5051\n",
            "\n",
            "\n",
            "Train Epoch: 29 [16/253 (7%)]\tLoss: 0.445810\n",
            "\n",
            "Training set: Average loss: 0.4465\n",
            "Time taken for epoch =  11.694934968000325\n",
            "Validation set: Average loss: 0.5566\n",
            "\n",
            "\n",
            "Train Epoch: 30 [16/253 (7%)]\tLoss: 0.455331\n",
            "\n",
            "Training set: Average loss: 0.4660\n",
            "Time taken for epoch =  12.74503563899998\n",
            "Validation set: Average loss: 0.6123\n",
            "\n",
            "\n",
            "Train Epoch: 31 [16/253 (7%)]\tLoss: 0.447059\n",
            "\n",
            "Training set: Average loss: 0.4462\n",
            "Time taken for epoch =  11.084260214000096\n",
            "Validation set: Average loss: 0.5372\n",
            "\n",
            "\n",
            "Train Epoch: 32 [16/253 (7%)]\tLoss: 0.404790\n",
            "\n",
            "Training set: Average loss: 0.4582\n",
            "Time taken for epoch =  12.352870075000283\n",
            "Validation set: Average loss: 0.4451\n",
            "\n",
            "\n",
            "Train Epoch: 33 [16/253 (7%)]\tLoss: 0.410454\n",
            "\n",
            "Training set: Average loss: 0.4386\n",
            "Time taken for epoch =  11.911403762999726\n",
            "Validation set: Average loss: 0.5867\n",
            "\n",
            "\n",
            "Train Epoch: 34 [16/253 (7%)]\tLoss: 0.469047\n",
            "\n",
            "Training set: Average loss: 0.4425\n",
            "Time taken for epoch =  10.446778443000312\n",
            "Validation set: Average loss: 0.7417\n",
            "\n",
            "\n",
            "Train Epoch: 35 [16/253 (7%)]\tLoss: 0.767371\n",
            "\n",
            "Training set: Average loss: 0.5537\n",
            "Time taken for epoch =  12.386047006999888\n",
            "Validation set: Average loss: 0.6533\n",
            "\n",
            "\n",
            "Train Epoch: 36 [16/253 (7%)]\tLoss: 0.548064\n",
            "\n",
            "Training set: Average loss: 0.5133\n",
            "Time taken for epoch =  13.379424665999977\n",
            "Validation set: Average loss: 0.7398\n",
            "\n",
            "\n",
            "Train Epoch: 37 [16/253 (7%)]\tLoss: 0.433017\n",
            "\n",
            "Training set: Average loss: 0.4345\n",
            "Time taken for epoch =  10.738422099000218\n",
            "Validation set: Average loss: 0.6075\n",
            "\n",
            "\n",
            "Train Epoch: 38 [16/253 (7%)]\tLoss: 0.418024\n",
            "\n",
            "Training set: Average loss: 0.4147\n",
            "Time taken for epoch =  12.357232851999925\n",
            "Validation set: Average loss: 0.4550\n",
            "\n",
            "\n",
            "Train Epoch: 39 [16/253 (7%)]\tLoss: 0.398098\n",
            "\n",
            "Training set: Average loss: 0.5702\n",
            "Time taken for epoch =  10.289950693000264\n",
            "Validation set: Average loss: 0.5674\n",
            "\n",
            "\n",
            "Train Epoch: 40 [16/253 (7%)]\tLoss: 0.536430\n",
            "\n",
            "Training set: Average loss: 0.5284\n",
            "Time taken for epoch =  11.831159380999907\n",
            "Validation set: Average loss: 0.5619\n",
            "\n",
            "\n",
            "Train Epoch: 41 [16/253 (7%)]\tLoss: 0.520033\n",
            "\n",
            "Training set: Average loss: 0.4983\n",
            "Time taken for epoch =  12.315454836999834\n",
            "Validation set: Average loss: 0.5469\n",
            "\n",
            "\n",
            "Train Epoch: 42 [16/253 (7%)]\tLoss: 0.494288\n",
            "\n",
            "Training set: Average loss: 0.4926\n",
            "Time taken for epoch =  12.394880900999851\n",
            "Validation set: Average loss: 0.5050\n",
            "\n",
            "\n",
            "Train Epoch: 43 [16/253 (7%)]\tLoss: 0.483178\n",
            "\n",
            "Training set: Average loss: 0.5497\n",
            "Time taken for epoch =  11.429470435999974\n",
            "Validation set: Average loss: 0.9655\n",
            "\n",
            "\n",
            "Train Epoch: 44 [16/253 (7%)]\tLoss: 0.561841\n",
            "\n",
            "Training set: Average loss: 0.5377\n",
            "Time taken for epoch =  10.830207187999804\n",
            "Validation set: Average loss: 0.5858\n",
            "\n",
            "\n",
            "Train Epoch: 45 [16/253 (7%)]\tLoss: 0.513116\n",
            "\n",
            "Training set: Average loss: 0.5256\n",
            "Time taken for epoch =  10.304570760999923\n",
            "Validation set: Average loss: 0.5514\n",
            "\n",
            "\n",
            "Train Epoch: 46 [16/253 (7%)]\tLoss: 0.509996\n",
            "\n",
            "Training set: Average loss: 0.5427\n",
            "Time taken for epoch =  12.305108392999955\n",
            "Validation set: Average loss: 0.8453\n",
            "\n",
            "\n",
            "Train Epoch: 47 [16/253 (7%)]\tLoss: 0.613464\n",
            "\n",
            "Training set: Average loss: 0.5463\n",
            "Time taken for epoch =  12.034289834999981\n",
            "Validation set: Average loss: 0.5990\n",
            "\n",
            "\n",
            "Train Epoch: 48 [16/253 (7%)]\tLoss: 0.507465\n",
            "\n",
            "Training set: Average loss: 0.5251\n",
            "Time taken for epoch =  12.287847642000088\n",
            "Validation set: Average loss: 0.5860\n",
            "\n",
            "\n",
            "Train Epoch: 49 [16/253 (7%)]\tLoss: 0.499548\n",
            "\n",
            "Training set: Average loss: 0.5451\n",
            "Time taken for epoch =  12.577122882000367\n",
            "Validation set: Average loss: 0.6490\n",
            "\n",
            "\n",
            "Train Epoch: 50 [16/253 (7%)]\tLoss: 0.507273\n",
            "\n",
            "Training set: Average loss: 0.5010\n",
            "Time taken for epoch =  12.107298834000176\n",
            "Validation set: Average loss: 0.5503\n",
            "\n",
            "\n",
            "Train Epoch: 51 [16/253 (7%)]\tLoss: 0.485638\n",
            "\n",
            "Training set: Average loss: 0.4949\n",
            "Time taken for epoch =  11.871620764\n",
            "Validation set: Average loss: 0.5933\n",
            "\n",
            "\n",
            "Train Epoch: 52 [16/253 (7%)]\tLoss: 0.544829\n",
            "\n",
            "Training set: Average loss: 0.5341\n",
            "Time taken for epoch =  11.598251911999796\n",
            "Validation set: Average loss: 0.5706\n",
            "\n",
            "\n",
            "Train Epoch: 53 [16/253 (7%)]\tLoss: 0.541741\n",
            "\n",
            "Training set: Average loss: 0.5102\n",
            "Time taken for epoch =  12.170205872000224\n",
            "Validation set: Average loss: 0.5233\n",
            "\n",
            "\n",
            "Train Epoch: 54 [16/253 (7%)]\tLoss: 0.508403\n",
            "\n",
            "Training set: Average loss: 0.5047\n",
            "Time taken for epoch =  24.660877938999874\n",
            "Validation set: Average loss: 0.5559\n",
            "\n",
            "\n",
            "Train Epoch: 55 [16/253 (7%)]\tLoss: 0.524180\n",
            "\n",
            "Training set: Average loss: 0.4938\n",
            "Time taken for epoch =  13.525658763000138\n",
            "Validation set: Average loss: 0.5194\n",
            "\n",
            "\n",
            "Train Epoch: 56 [16/253 (7%)]\tLoss: 0.457453\n",
            "\n",
            "Training set: Average loss: 0.4902\n",
            "Time taken for epoch =  11.588939750999998\n",
            "Validation set: Average loss: 0.7225\n",
            "\n",
            "\n",
            "Train Epoch: 57 [16/253 (7%)]\tLoss: 0.608324\n",
            "\n",
            "Training set: Average loss: 0.5082\n",
            "Time taken for epoch =  10.408369650000168\n",
            "Validation set: Average loss: 0.5052\n",
            "\n",
            "\n",
            "Train Epoch: 58 [16/253 (7%)]\tLoss: 0.456898\n",
            "\n",
            "Training set: Average loss: 0.4815\n",
            "Time taken for epoch =  12.088292553999963\n",
            "Validation set: Average loss: 0.5042\n",
            "\n",
            "\n",
            "Train Epoch: 59 [16/253 (7%)]\tLoss: 0.448003\n",
            "\n",
            "Training set: Average loss: 0.4671\n",
            "Time taken for epoch =  12.302534946999913\n",
            "Validation set: Average loss: 0.5207\n",
            "\n",
            "\n",
            "Train Epoch: 60 [16/253 (7%)]\tLoss: 0.470231\n",
            "\n",
            "Training set: Average loss: 0.4790\n",
            "Time taken for epoch =  12.275864749999982\n",
            "Validation set: Average loss: 0.4874\n",
            "\n",
            "\n",
            "Train Epoch: 61 [16/253 (7%)]\tLoss: 0.524415\n",
            "\n",
            "Training set: Average loss: 0.4756\n",
            "Time taken for epoch =  12.043930105000072\n",
            "Validation set: Average loss: 0.6273\n",
            "\n",
            "\n",
            "Train Epoch: 62 [16/253 (7%)]\tLoss: 0.493932\n",
            "\n",
            "Training set: Average loss: 0.4936\n",
            "Time taken for epoch =  11.70068854100009\n",
            "Validation set: Average loss: 0.5088\n",
            "\n",
            "\n",
            "Train Epoch: 63 [16/253 (7%)]\tLoss: 0.438253\n",
            "\n",
            "Training set: Average loss: 0.4804\n",
            "Time taken for epoch =  12.21700263899993\n",
            "Validation set: Average loss: 0.5156\n",
            "\n",
            "\n",
            "Train Epoch: 64 [16/253 (7%)]\tLoss: 0.481533\n",
            "\n",
            "Training set: Average loss: 0.4579\n",
            "Time taken for epoch =  11.361449234000247\n",
            "Validation set: Average loss: 0.4881\n",
            "\n",
            "\n",
            "Train Epoch: 65 [16/253 (7%)]\tLoss: 0.444276\n",
            "\n",
            "Training set: Average loss: 0.4600\n",
            "Time taken for epoch =  12.283414814000025\n",
            "Validation set: Average loss: 0.5210\n",
            "\n",
            "\n",
            "Train Epoch: 66 [16/253 (7%)]\tLoss: 0.422787\n",
            "\n",
            "Training set: Average loss: 0.4501\n",
            "Time taken for epoch =  11.707992104999903\n",
            "Validation set: Average loss: 0.4832\n",
            "\n",
            "\n",
            "Train Epoch: 67 [16/253 (7%)]\tLoss: 0.441111\n",
            "\n",
            "Training set: Average loss: 0.5034\n",
            "Time taken for epoch =  11.697404314000323\n",
            "Validation set: Average loss: 0.5015\n",
            "\n",
            "\n",
            "Train Epoch: 68 [16/253 (7%)]\tLoss: 0.488790\n",
            "\n",
            "Training set: Average loss: 0.4604\n",
            "Time taken for epoch =  11.717352090000077\n",
            "Validation set: Average loss: 0.5340\n",
            "\n",
            "\n",
            "Train Epoch: 69 [16/253 (7%)]\tLoss: 0.467247\n",
            "\n",
            "Training set: Average loss: 0.4556\n",
            "Time taken for epoch =  11.471565244999965\n",
            "Validation set: Average loss: 0.4749\n",
            "\n",
            "\n",
            "Train Epoch: 70 [16/253 (7%)]\tLoss: 0.443110\n",
            "\n",
            "Training set: Average loss: 0.4696\n",
            "Time taken for epoch =  11.589425818000109\n",
            "Validation set: Average loss: 0.4926\n",
            "\n",
            "\n",
            "Train Epoch: 71 [16/253 (7%)]\tLoss: 0.443670\n",
            "\n",
            "Training set: Average loss: 0.4502\n",
            "Time taken for epoch =  12.114286355000331\n",
            "Validation set: Average loss: 0.4520\n",
            "\n",
            "\n",
            "Train Epoch: 72 [16/253 (7%)]\tLoss: 0.428063\n",
            "\n",
            "Training set: Average loss: 0.4591\n",
            "Time taken for epoch =  12.125263991999873\n",
            "Validation set: Average loss: 0.6968\n",
            "\n",
            "\n",
            "Train Epoch: 73 [16/253 (7%)]\tLoss: 0.476711\n",
            "\n",
            "Training set: Average loss: 0.4450\n",
            "Time taken for epoch =  11.60278375100006\n",
            "Validation set: Average loss: 0.5936\n",
            "\n",
            "\n",
            "Train Epoch: 74 [16/253 (7%)]\tLoss: 0.434697\n",
            "\n",
            "Training set: Average loss: 0.4597\n",
            "Time taken for epoch =  11.791578286000004\n",
            "Validation set: Average loss: 0.6494\n",
            "\n",
            "\n",
            "Train Epoch: 75 [16/253 (7%)]\tLoss: 0.504050\n",
            "\n",
            "Training set: Average loss: 0.5349\n",
            "Time taken for epoch =  12.250480920000427\n",
            "Validation set: Average loss: 0.6001\n",
            "\n",
            "\n",
            "Train Epoch: 76 [16/253 (7%)]\tLoss: 0.495165\n",
            "\n",
            "Training set: Average loss: 0.4979\n",
            "Time taken for epoch =  10.568924616000004\n",
            "Validation set: Average loss: 0.5102\n",
            "\n",
            "\n",
            "Train Epoch: 77 [16/253 (7%)]\tLoss: 0.507852\n",
            "\n",
            "Training set: Average loss: 0.4788\n",
            "Time taken for epoch =  11.157031339000241\n",
            "Validation set: Average loss: 0.6263\n",
            "\n",
            "\n",
            "Train Epoch: 78 [16/253 (7%)]\tLoss: 0.461904\n",
            "\n",
            "Training set: Average loss: 0.4610\n",
            "Time taken for epoch =  12.050805989999844\n",
            "Validation set: Average loss: 0.5018\n",
            "\n",
            "\n",
            "Train Epoch: 79 [16/253 (7%)]\tLoss: 0.476648\n",
            "\n",
            "Training set: Average loss: 0.4494\n",
            "Time taken for epoch =  11.130526645000373\n",
            "Validation set: Average loss: 0.6164\n",
            "\n",
            "\n",
            "Train Epoch: 80 [16/253 (7%)]\tLoss: 0.451555\n",
            "\n",
            "Training set: Average loss: 0.4646\n",
            "Time taken for epoch =  11.032254505999845\n",
            "Validation set: Average loss: 0.5410\n",
            "\n",
            "\n",
            "Train Epoch: 81 [16/253 (7%)]\tLoss: 0.449761\n",
            "\n",
            "Training set: Average loss: 0.5233\n",
            "Time taken for epoch =  10.898834490999889\n",
            "Validation set: Average loss: 0.5131\n",
            "\n",
            "\n",
            "Train Epoch: 82 [16/253 (7%)]\tLoss: 0.456876\n",
            "\n",
            "Training set: Average loss: 0.4829\n",
            "Time taken for epoch =  12.390802016999714\n",
            "Validation set: Average loss: 0.6096\n",
            "\n",
            "\n",
            "Train Epoch: 83 [16/253 (7%)]\tLoss: 0.517065\n",
            "\n",
            "Training set: Average loss: 0.4977\n",
            "Time taken for epoch =  12.14507483899979\n",
            "Validation set: Average loss: 0.6440\n",
            "\n",
            "\n",
            "Train Epoch: 84 [16/253 (7%)]\tLoss: 0.450509\n",
            "\n",
            "Training set: Average loss: 0.4789\n",
            "Time taken for epoch =  11.95610223499989\n",
            "Validation set: Average loss: 0.6435\n",
            "\n",
            "\n",
            "Train Epoch: 85 [16/253 (7%)]\tLoss: 0.465892\n",
            "\n",
            "Training set: Average loss: 0.4893\n",
            "Time taken for epoch =  11.520328281000275\n",
            "Validation set: Average loss: 0.7409\n",
            "\n",
            "\n",
            "Train Epoch: 86 [16/253 (7%)]\tLoss: 0.562016\n",
            "\n",
            "Training set: Average loss: 0.4906\n",
            "Time taken for epoch =  11.406597754999893\n",
            "Validation set: Average loss: 0.4824\n",
            "\n",
            "\n",
            "Train Epoch: 87 [16/253 (7%)]\tLoss: 0.469551\n",
            "\n",
            "Training set: Average loss: 0.4569\n",
            "Time taken for epoch =  11.847730450999734\n",
            "Validation set: Average loss: 0.6598\n",
            "\n",
            "\n",
            "Train Epoch: 88 [16/253 (7%)]\tLoss: 0.476430\n",
            "\n",
            "Training set: Average loss: 0.4503\n",
            "Time taken for epoch =  10.490299261000473\n",
            "Validation set: Average loss: 0.5213\n",
            "\n",
            "\n",
            "Train Epoch: 89 [16/253 (7%)]\tLoss: 0.446253\n",
            "\n",
            "Training set: Average loss: 0.4910\n",
            "Time taken for epoch =  10.613880471000812\n",
            "Validation set: Average loss: 0.5163\n",
            "\n",
            "\n",
            "Train Epoch: 90 [16/253 (7%)]\tLoss: 0.492419\n",
            "\n",
            "Training set: Average loss: 0.4903\n",
            "Time taken for epoch =  11.701967748000243\n",
            "Validation set: Average loss: 0.5085\n",
            "\n",
            "\n",
            "Train Epoch: 91 [16/253 (7%)]\tLoss: 0.432340\n",
            "\n",
            "Training set: Average loss: 0.5154\n",
            "Time taken for epoch =  11.36687626299954\n",
            "Validation set: Average loss: 0.5530\n",
            "\n",
            "\n",
            "Train Epoch: 92 [16/253 (7%)]\tLoss: 0.490939\n",
            "\n",
            "Training set: Average loss: 0.4688\n",
            "Time taken for epoch =  10.694209887000397\n",
            "Validation set: Average loss: 0.7428\n",
            "\n",
            "\n",
            "Train Epoch: 93 [16/253 (7%)]\tLoss: 0.446007\n",
            "\n",
            "Training set: Average loss: 0.4569\n",
            "Time taken for epoch =  11.236332646999472\n",
            "Validation set: Average loss: 1.1000\n",
            "\n",
            "\n",
            "Train Epoch: 94 [16/253 (7%)]\tLoss: 0.481705\n",
            "\n",
            "Training set: Average loss: 0.4836\n",
            "Time taken for epoch =  10.548689082999772\n",
            "Validation set: Average loss: 0.5325\n",
            "\n",
            "\n",
            "Train Epoch: 95 [16/253 (7%)]\tLoss: 0.547885\n",
            "\n",
            "Training set: Average loss: 0.4883\n",
            "Time taken for epoch =  11.052695076999953\n",
            "Validation set: Average loss: 0.6259\n",
            "\n",
            "\n",
            "Train Epoch: 96 [16/253 (7%)]\tLoss: 0.450537\n",
            "\n",
            "Training set: Average loss: 0.4602\n",
            "Time taken for epoch =  12.211036771999716\n",
            "Validation set: Average loss: 0.9913\n",
            "\n",
            "\n",
            "Train Epoch: 97 [16/253 (7%)]\tLoss: 0.454501\n",
            "\n",
            "Training set: Average loss: 0.4712\n",
            "Time taken for epoch =  11.2987214449995\n",
            "Validation set: Average loss: 0.5489\n",
            "\n",
            "\n",
            "Train Epoch: 98 [16/253 (7%)]\tLoss: 0.530155\n",
            "\n",
            "Training set: Average loss: 0.4985\n",
            "Time taken for epoch =  11.189228158000333\n",
            "Validation set: Average loss: 0.5761\n",
            "\n",
            "\n",
            "Train Epoch: 99 [16/253 (7%)]\tLoss: 0.476501\n",
            "\n",
            "Training set: Average loss: 0.4589\n",
            "Time taken for epoch =  11.952217791000294\n",
            "Validation set: Average loss: 0.7057\n",
            "\n",
            "\n",
            "Train Epoch: 100 [16/253 (7%)]\tLoss: 0.482985\n",
            "\n",
            "Training set: Average loss: 0.4681\n",
            "Time taken for epoch =  11.325817283000106\n",
            "Validation set: Average loss: 0.4750\n",
            "\n",
            "\n",
            "Train Epoch: 101 [16/253 (7%)]\tLoss: 0.421380\n",
            "\n",
            "Training set: Average loss: 0.4878\n",
            "Time taken for epoch =  11.419057314999918\n",
            "Validation set: Average loss: 0.5015\n",
            "\n",
            "\n",
            "Train Epoch: 102 [16/253 (7%)]\tLoss: 0.471301\n",
            "\n",
            "Training set: Average loss: 0.4621\n",
            "Time taken for epoch =  12.130259083000055\n",
            "Validation set: Average loss: 0.6349\n",
            "\n",
            "\n",
            "Train Epoch: 103 [16/253 (7%)]\tLoss: 0.442455\n",
            "\n",
            "Training set: Average loss: 0.4461\n",
            "Time taken for epoch =  16.08568306699999\n",
            "Validation set: Average loss: 0.7595\n",
            "\n",
            "\n",
            "Train Epoch: 104 [16/253 (7%)]\tLoss: 0.430315\n",
            "\n",
            "Training set: Average loss: 0.4920\n",
            "Time taken for epoch =  17.82996065900079\n",
            "Validation set: Average loss: 0.7750\n",
            "\n",
            "\n",
            "Train Epoch: 105 [16/253 (7%)]\tLoss: 0.574155\n",
            "\n",
            "Training set: Average loss: 0.5065\n",
            "Time taken for epoch =  11.518649687000106\n",
            "Validation set: Average loss: 0.5515\n",
            "\n",
            "\n",
            "Train Epoch: 106 [16/253 (7%)]\tLoss: 0.477685\n",
            "\n",
            "Training set: Average loss: 0.5440\n",
            "Time taken for epoch =  12.140265512999576\n",
            "Validation set: Average loss: 0.5985\n",
            "\n",
            "\n",
            "Train Epoch: 107 [16/253 (7%)]\tLoss: 0.524493\n",
            "\n",
            "Training set: Average loss: 0.5483\n",
            "Time taken for epoch =  11.182918164000512\n",
            "Validation set: Average loss: 0.7389\n",
            "\n",
            "\n",
            "Train Epoch: 108 [16/253 (7%)]\tLoss: 0.603904\n",
            "\n",
            "Training set: Average loss: 0.5140\n",
            "Time taken for epoch =  11.471665592000136\n",
            "Validation set: Average loss: 0.6649\n",
            "\n",
            "\n",
            "Train Epoch: 109 [16/253 (7%)]\tLoss: 0.493533\n",
            "\n",
            "Training set: Average loss: 0.5147\n",
            "Time taken for epoch =  11.862818704999881\n",
            "Validation set: Average loss: 0.5307\n",
            "\n",
            "\n",
            "Train Epoch: 110 [16/253 (7%)]\tLoss: 0.480305\n",
            "\n",
            "Training set: Average loss: 0.5017\n",
            "Time taken for epoch =  10.544662582000456\n",
            "Validation set: Average loss: 0.5249\n",
            "\n",
            "\n",
            "Train Epoch: 111 [16/253 (7%)]\tLoss: 0.511284\n",
            "\n",
            "Training set: Average loss: 0.4791\n",
            "Time taken for epoch =  11.078463297999406\n",
            "Validation set: Average loss: 0.5008\n",
            "\n",
            "\n",
            "Train Epoch: 112 [16/253 (7%)]\tLoss: 0.461910\n",
            "\n",
            "Training set: Average loss: 0.4962\n",
            "Time taken for epoch =  12.24931364099939\n",
            "Validation set: Average loss: 0.5064\n",
            "\n",
            "\n",
            "Train Epoch: 113 [16/253 (7%)]\tLoss: 0.456686\n",
            "\n",
            "Training set: Average loss: 0.5065\n",
            "Time taken for epoch =  11.173423552999338\n",
            "Validation set: Average loss: 0.5145\n",
            "\n",
            "\n",
            "Train Epoch: 114 [16/253 (7%)]\tLoss: 0.493984\n",
            "\n",
            "Training set: Average loss: 0.4894\n",
            "Time taken for epoch =  12.103693916999873\n",
            "Validation set: Average loss: 0.5110\n",
            "\n",
            "\n",
            "Train Epoch: 115 [16/253 (7%)]\tLoss: 0.462703\n",
            "\n",
            "Training set: Average loss: 0.4992\n",
            "Time taken for epoch =  11.045507539000027\n",
            "Validation set: Average loss: 0.5649\n",
            "\n",
            "\n",
            "Train Epoch: 116 [16/253 (7%)]\tLoss: 0.470112\n",
            "\n",
            "Training set: Average loss: 0.4947\n",
            "Time taken for epoch =  12.791333924000355\n",
            "Validation set: Average loss: 0.5094\n",
            "\n",
            "\n",
            "Train Epoch: 117 [16/253 (7%)]\tLoss: 0.487798\n",
            "\n",
            "Training set: Average loss: 0.4743\n",
            "Time taken for epoch =  11.667631786999664\n",
            "Validation set: Average loss: 0.4812\n",
            "\n",
            "\n",
            "Train Epoch: 118 [16/253 (7%)]\tLoss: 0.471470\n",
            "\n",
            "Training set: Average loss: 0.4719\n",
            "Time taken for epoch =  12.122263110999484\n",
            "Validation set: Average loss: 0.4735\n",
            "\n",
            "\n",
            "Train Epoch: 119 [16/253 (7%)]\tLoss: 0.432814\n",
            "\n",
            "Training set: Average loss: 0.4403\n",
            "Time taken for epoch =  11.602690097999584\n",
            "Validation set: Average loss: 0.5430\n",
            "\n",
            "\n",
            "Train Epoch: 120 [16/253 (7%)]\tLoss: 0.472424\n",
            "\n",
            "Training set: Average loss: 0.4626\n",
            "Time taken for epoch =  10.676185528000133\n",
            "Validation set: Average loss: 0.8420\n",
            "\n",
            "\n",
            "Train Epoch: 121 [16/253 (7%)]\tLoss: 0.485397\n",
            "\n",
            "Training set: Average loss: 0.4426\n",
            "Time taken for epoch =  11.81981172399992\n",
            "Validation set: Average loss: 0.5734\n",
            "\n",
            "\n",
            "Train Epoch: 122 [16/253 (7%)]\tLoss: 0.513330\n",
            "\n",
            "Training set: Average loss: 0.4934\n",
            "Time taken for epoch =  10.89945675799936\n",
            "Validation set: Average loss: 0.5074\n",
            "\n",
            "\n",
            "Train Epoch: 123 [16/253 (7%)]\tLoss: 0.478058\n",
            "\n",
            "Training set: Average loss: 0.5011\n",
            "Time taken for epoch =  11.752597312000034\n",
            "Validation set: Average loss: 0.5533\n",
            "\n",
            "\n",
            "Train Epoch: 124 [16/253 (7%)]\tLoss: 0.489910\n",
            "\n",
            "Training set: Average loss: 0.4892\n",
            "Time taken for epoch =  11.532676627\n",
            "Validation set: Average loss: 0.5076\n",
            "\n",
            "\n",
            "Train Epoch: 125 [16/253 (7%)]\tLoss: 0.434499\n",
            "\n",
            "Training set: Average loss: 0.4476\n",
            "Time taken for epoch =  12.04927807599961\n",
            "Validation set: Average loss: 0.5996\n",
            "\n",
            "\n",
            "Train Epoch: 126 [16/253 (7%)]\tLoss: 0.451016\n",
            "\n",
            "Training set: Average loss: 0.4770\n",
            "Time taken for epoch =  11.860945325000102\n",
            "Validation set: Average loss: 0.6039\n",
            "\n",
            "\n",
            "Train Epoch: 127 [16/253 (7%)]\tLoss: 0.552369\n",
            "\n",
            "Training set: Average loss: 0.4640\n",
            "Time taken for epoch =  10.55807886599996\n",
            "Validation set: Average loss: 0.7223\n",
            "\n",
            "\n",
            "Train Epoch: 128 [16/253 (7%)]\tLoss: 0.718834\n",
            "\n",
            "Training set: Average loss: 0.5181\n",
            "Time taken for epoch =  11.019427052000538\n",
            "Validation set: Average loss: 0.6447\n",
            "\n",
            "\n",
            "Train Epoch: 129 [16/253 (7%)]\tLoss: 0.546736\n",
            "\n",
            "Training set: Average loss: 0.4947\n",
            "Time taken for epoch =  10.895923813999616\n",
            "Validation set: Average loss: 0.4892\n",
            "\n",
            "\n",
            "Train Epoch: 130 [16/253 (7%)]\tLoss: 0.451219\n",
            "\n",
            "Training set: Average loss: 0.4917\n",
            "Time taken for epoch =  11.960861639000541\n",
            "Validation set: Average loss: 0.6031\n",
            "\n",
            "\n",
            "Train Epoch: 131 [16/253 (7%)]\tLoss: 0.443480\n",
            "\n",
            "Training set: Average loss: 0.4644\n",
            "Time taken for epoch =  10.742409801000576\n",
            "Validation set: Average loss: 0.4963\n",
            "\n",
            "\n",
            "Train Epoch: 132 [16/253 (7%)]\tLoss: 0.427373\n",
            "\n",
            "Training set: Average loss: 0.4767\n",
            "Time taken for epoch =  9.754511762999755\n",
            "Validation set: Average loss: 0.5102\n",
            "\n",
            "\n",
            "Train Epoch: 133 [16/253 (7%)]\tLoss: 0.476812\n",
            "\n",
            "Training set: Average loss: 0.4894\n",
            "Time taken for epoch =  11.956486750000295\n",
            "Validation set: Average loss: 0.5016\n",
            "\n",
            "\n",
            "Train Epoch: 134 [16/253 (7%)]\tLoss: 0.458143\n",
            "\n",
            "Training set: Average loss: 0.4447\n",
            "Time taken for epoch =  12.13968352899974\n",
            "Validation set: Average loss: 0.4995\n",
            "\n",
            "\n",
            "Train Epoch: 135 [16/253 (7%)]\tLoss: 0.451648\n",
            "\n",
            "Training set: Average loss: 0.4717\n",
            "Time taken for epoch =  12.071636398999544\n",
            "Validation set: Average loss: 0.4796\n",
            "\n",
            "\n",
            "Train Epoch: 136 [16/253 (7%)]\tLoss: 0.423155\n",
            "\n",
            "Training set: Average loss: 0.4465\n",
            "Time taken for epoch =  12.80166673399981\n",
            "Validation set: Average loss: 0.7277\n",
            "\n",
            "\n",
            "Train Epoch: 137 [16/253 (7%)]\tLoss: 0.442885\n",
            "\n",
            "Training set: Average loss: 0.4656\n",
            "Time taken for epoch =  11.192701550000493\n",
            "Validation set: Average loss: 0.7826\n",
            "\n",
            "\n",
            "Train Epoch: 138 [16/253 (7%)]\tLoss: 0.463349\n",
            "\n",
            "Training set: Average loss: 0.4350\n",
            "Time taken for epoch =  11.501878099999885\n",
            "Validation set: Average loss: 0.7114\n",
            "\n",
            "\n",
            "Train Epoch: 139 [16/253 (7%)]\tLoss: 0.569329\n",
            "\n",
            "Training set: Average loss: 0.4496\n",
            "Time taken for epoch =  12.049721217999831\n",
            "Validation set: Average loss: 0.5960\n",
            "\n",
            "\n",
            "Train Epoch: 140 [16/253 (7%)]\tLoss: 0.436097\n",
            "\n",
            "Training set: Average loss: 0.4466\n",
            "Time taken for epoch =  11.5343197279999\n",
            "Validation set: Average loss: 0.4779\n",
            "\n",
            "\n",
            "Train Epoch: 141 [16/253 (7%)]\tLoss: 0.422063\n",
            "\n",
            "Training set: Average loss: 0.4483\n",
            "Time taken for epoch =  11.898688866999692\n",
            "Validation set: Average loss: 0.6765\n",
            "\n",
            "\n",
            "Train Epoch: 142 [16/253 (7%)]\tLoss: 0.423861\n",
            "\n",
            "Training set: Average loss: 0.4751\n",
            "Time taken for epoch =  11.950620282000273\n",
            "Validation set: Average loss: 0.5062\n",
            "\n",
            "\n",
            "Train Epoch: 143 [16/253 (7%)]\tLoss: 0.468569\n",
            "\n",
            "Training set: Average loss: 0.5179\n",
            "Time taken for epoch =  11.262023086000227\n",
            "Validation set: Average loss: 0.5190\n",
            "\n",
            "\n",
            "Train Epoch: 144 [16/253 (7%)]\tLoss: 0.483537\n",
            "\n",
            "Training set: Average loss: 0.4992\n",
            "Time taken for epoch =  11.655486347000078\n",
            "Validation set: Average loss: 0.5918\n",
            "\n",
            "\n",
            "Train Epoch: 145 [16/253 (7%)]\tLoss: 0.511225\n",
            "\n",
            "Training set: Average loss: 0.5019\n",
            "Time taken for epoch =  11.0122711289996\n",
            "Validation set: Average loss: 0.6087\n",
            "\n",
            "\n",
            "Train Epoch: 146 [16/253 (7%)]\tLoss: 0.539021\n",
            "\n",
            "Training set: Average loss: 0.4788\n",
            "Time taken for epoch =  12.279256883999551\n",
            "Validation set: Average loss: 0.5639\n",
            "\n",
            "\n",
            "Train Epoch: 147 [16/253 (7%)]\tLoss: 0.455982\n",
            "\n",
            "Training set: Average loss: 0.4680\n",
            "Time taken for epoch =  11.011761417999878\n",
            "Validation set: Average loss: 0.5685\n",
            "\n",
            "\n",
            "Train Epoch: 148 [16/253 (7%)]\tLoss: 0.492346\n",
            "\n",
            "Training set: Average loss: 0.4857\n",
            "Time taken for epoch =  11.223234932000196\n",
            "Validation set: Average loss: 0.4824\n",
            "\n",
            "\n",
            "Train Epoch: 149 [16/253 (7%)]\tLoss: 0.438593\n",
            "\n",
            "Training set: Average loss: 0.4699\n",
            "Time taken for epoch =  11.298170517000472\n",
            "Validation set: Average loss: 0.6379\n",
            "\n",
            "\n",
            "Train Epoch: 150 [16/253 (7%)]\tLoss: 0.449423\n",
            "\n",
            "Training set: Average loss: 0.4570\n",
            "Time taken for epoch =  11.841758540999763\n",
            "Validation set: Average loss: 0.7786\n",
            "\n",
            "\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 19 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 19 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/DLH-Miniproject/e/DLHMIN-63/metadata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), \"UNet.pth\")\n",
        "run[\"model_checkpoint/final_model\"].upload(\"UNet.pth\")"
      ],
      "metadata": {
        "id": "LlhK_0jZX3dN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I71HHuGwxizR",
        "outputId": "174ea90a-1054-4a26-9dab-0e5c2dcb6fe1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 2 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 2 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/DLH-Miniproject/e/DLHMIN-61/metadata\n"
          ]
        }
      ]
    }
  ]
}